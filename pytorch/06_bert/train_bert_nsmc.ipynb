{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7ad3f7f",
   "metadata": {},
   "source": [
    "- notebook : https://github.com/donarts/sourcecode/blob/main/pytorch/06_bert\n",
    "- train : https://swlock.blogspot.com/2023/03/bert-nsmc-pytorch-train.html\n",
    "- predict : https://swlock.blogspot.com/2023/03/bert-nsmc-pytorch-predict.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7d55f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Korpora import Korpora\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a09a8f5",
   "metadata": {},
   "source": [
    "fineturningí•  nsmc ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a05b455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora ëŠ” ë‹¤ë¥¸ ë¶„ë“¤ì´ ì—°êµ¬ ëª©ì ìœ¼ë¡œ ê³µìœ í•´ì£¼ì‹  ë§ë­‰ì¹˜ë“¤ì„\n",
      "    ì†ì‰½ê²Œ ë‹¤ìš´ë¡œë“œ, ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ë§Œì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "    ë§ë­‰ì¹˜ë“¤ì„ ê³µìœ í•´ ì£¼ì‹  ë¶„ë“¤ì—ê²Œ ê°ì‚¬ë“œë¦¬ë©°, ê° ë§ë­‰ì¹˜ ë³„ ì„¤ëª…ê³¼ ë¼ì´ì„¼ìŠ¤ë¥¼ ê³µìœ  ë“œë¦½ë‹ˆë‹¤.\n",
      "    í•´ë‹¹ ë§ë­‰ì¹˜ì— ëŒ€í•´ ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹  ë¶„ì€ ì•„ë˜ì˜ description ì„ ì°¸ê³ ,\n",
      "    í•´ë‹¹ ë§ë­‰ì¹˜ë¥¼ ì—°êµ¬/ìƒìš©ì˜ ëª©ì ìœ¼ë¡œ ì´ìš©í•˜ì‹¤ ë•Œì—ëŠ” ì•„ë˜ì˜ ë¼ì´ì„¼ìŠ¤ë¥¼ ì°¸ê³ í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\jun\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\jun\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "NSMC = Korpora.load('nsmc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913825b6",
   "metadata": {},
   "source": [
    "dataframe ì— ë„£ì–´ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5efaf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({\"texts\":NSMC.train.texts, \"labels\":NSMC.train.labels})\n",
    "test_data = pd.DataFrame({\"texts\":NSMC.test.texts, \"labels\":NSMC.test.labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a09fec23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>ì¸ê°„ì´ ë¬¸ì œì§€.. ì†ŒëŠ” ë­”ì£„ì¸ê°€..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>í‰ì ì´ ë„ˆë¬´ ë‚®ì•„ì„œ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>ì´ê²Œ ë­ìš”? í•œêµ­ì¸ì€ ê±°ë“¤ë¨¹ê±°ë¦¬ê³  í•„ë¦¬í•€ í˜¼í˜ˆì€ ì°©í•˜ë‹¤?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>ì²­ì¶˜ ì˜í™”ì˜ ìµœê³ ë´‰.ë°©í™©ê³¼ ìš°ìš¸í–ˆë˜ ë‚ ë“¤ì˜ ìí™”ìƒ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>í•œêµ­ ì˜í™” ìµœì´ˆë¡œ ìˆ˜ê°„í•˜ëŠ” ë‚´ìš©ì´ ë‹´ê¸´ ì˜í™”</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    texts  labels\n",
       "0                                     ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬       0\n",
       "1                       í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜       1\n",
       "2                                       ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤       0\n",
       "3                           êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •       0\n",
       "4       ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...       1\n",
       "...                                                   ...     ...\n",
       "149995                                ì¸ê°„ì´ ë¬¸ì œì§€.. ì†ŒëŠ” ë­”ì£„ì¸ê°€..       0\n",
       "149996                                      í‰ì ì´ ë„ˆë¬´ ë‚®ì•„ì„œ...       1\n",
       "149997                    ì´ê²Œ ë­ìš”? í•œêµ­ì¸ì€ ê±°ë“¤ë¨¹ê±°ë¦¬ê³  í•„ë¦¬í•€ í˜¼í˜ˆì€ ì°©í•˜ë‹¤?       0\n",
       "149998                        ì²­ì¶˜ ì˜í™”ì˜ ìµœê³ ë´‰.ë°©í™©ê³¼ ìš°ìš¸í–ˆë˜ ë‚ ë“¤ì˜ ìí™”ìƒ       1\n",
       "149999                           í•œêµ­ ì˜í™” ìµœì´ˆë¡œ ìˆ˜ê°„í•˜ëŠ” ë‚´ìš©ì´ ë‹´ê¸´ ì˜í™”       0\n",
       "\n",
       "[150000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fccb66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>êµ³ ã…‹</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GDNTOPCLASSINTHECLUB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ë­ì•¼ ì´ í‰ì ë“¤ì€.... ë‚˜ì˜ì§„ ì•Šì§€ë§Œ 10ì  ì§œë¦¬ëŠ” ë”ë”ìš± ì•„ë‹ˆì–ì•„</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ì§€ë£¨í•˜ì§€ëŠ” ì•Šì€ë° ì™„ì „ ë§‰ì¥ì„... ëˆì£¼ê³  ë³´ê¸°ì—ëŠ”....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3Dë§Œ ì•„ë‹ˆì—ˆì–´ë„ ë³„ ë‹¤ì„¯ ê°œ ì¤¬ì„í…ë°.. ì™œ 3Dë¡œ ë‚˜ì™€ì„œ ì œ ì‹¬ê¸°ë¥¼ ë¶ˆí¸í•˜ê²Œ í•˜ì£ ??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>ì˜¤ëœë§Œì— í‰ì  ë¡œê¸´í–ˆë„¤ã…‹ã…‹ í‚¹ì™•ì§± ìŒˆë½•í•œ ì˜í™”ë¥¼ ë§Œë‚¬ìŠµë‹ˆë‹¤ ê°•ë ¬í•˜ê²Œ ìœ¡ì¾Œí•¨</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>ì˜ì§€ ë°•ì•½ë“¤ì´ë‚˜ í•˜ëŠ”ê±°ë‹¤ íƒˆì˜ì€ ì¼ë‹¨ ì£¼ì¸ê³µ ê¹€ëŒ€í¬ ë‹®ì•˜ê³  ì´ë“±ë³‘ ì°ë”° OOOO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>ê·¸ë¦¼ë„ ì¢‹ê³  ì™„ì„±ë„ë„ ë†’ì•˜ì§€ë§Œ... ë³´ëŠ” ë‚´ë‚´ ë¶ˆì•ˆí•˜ê²Œ ë§Œë“ ë‹¤</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>ì ˆëŒ€ ë´ì„œëŠ” ì•ˆ ë  ì˜í™”.. ì¬ë¯¸ë„ ì—†ê³  ê¸°ë¶„ë§Œ ì¡ì¹˜ê³ .. í•œ ì„¸íŠ¸ì¥ì—ì„œ ë‹¤ í•´ë¨¹ë„¤</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>ë§ˆë¬´ë¦¬ëŠ” ë˜ ì™œì´ë˜</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   texts  labels\n",
       "0                                                    êµ³ ã…‹       1\n",
       "1                                   GDNTOPCLASSINTHECLUB       0\n",
       "2                 ë­ì•¼ ì´ í‰ì ë“¤ì€.... ë‚˜ì˜ì§„ ì•Šì§€ë§Œ 10ì  ì§œë¦¬ëŠ” ë”ë”ìš± ì•„ë‹ˆì–ì•„       0\n",
       "3                       ì§€ë£¨í•˜ì§€ëŠ” ì•Šì€ë° ì™„ì „ ë§‰ì¥ì„... ëˆì£¼ê³  ë³´ê¸°ì—ëŠ”....       0\n",
       "4      3Dë§Œ ì•„ë‹ˆì—ˆì–´ë„ ë³„ ë‹¤ì„¯ ê°œ ì¤¬ì„í…ë°.. ì™œ 3Dë¡œ ë‚˜ì™€ì„œ ì œ ì‹¬ê¸°ë¥¼ ë¶ˆí¸í•˜ê²Œ í•˜ì£ ??       0\n",
       "...                                                  ...     ...\n",
       "49995          ì˜¤ëœë§Œì— í‰ì  ë¡œê¸´í–ˆë„¤ã…‹ã…‹ í‚¹ì™•ì§± ìŒˆë½•í•œ ì˜í™”ë¥¼ ë§Œë‚¬ìŠµë‹ˆë‹¤ ê°•ë ¬í•˜ê²Œ ìœ¡ì¾Œí•¨       1\n",
       "49996       ì˜ì§€ ë°•ì•½ë“¤ì´ë‚˜ í•˜ëŠ”ê±°ë‹¤ íƒˆì˜ì€ ì¼ë‹¨ ì£¼ì¸ê³µ ê¹€ëŒ€í¬ ë‹®ì•˜ê³  ì´ë“±ë³‘ ì°ë”° OOOO       0\n",
       "49997                 ê·¸ë¦¼ë„ ì¢‹ê³  ì™„ì„±ë„ë„ ë†’ì•˜ì§€ë§Œ... ë³´ëŠ” ë‚´ë‚´ ë¶ˆì•ˆí•˜ê²Œ ë§Œë“ ë‹¤       0\n",
       "49998     ì ˆëŒ€ ë´ì„œëŠ” ì•ˆ ë  ì˜í™”.. ì¬ë¯¸ë„ ì—†ê³  ê¸°ë¶„ë§Œ ì¡ì¹˜ê³ .. í•œ ì„¸íŠ¸ì¥ì—ì„œ ë‹¤ í•´ë¨¹ë„¤       0\n",
       "49999                                         ë§ˆë¬´ë¦¬ëŠ” ë˜ ì™œì´ë˜       0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2584c0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(l) for l in train_data['texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36c9201e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(l) for l in test_data['texts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfbfab7",
   "metadata": {},
   "source": [
    "ì—¬ê¸°ì—ì„œ train/test ë°ì´í„°ê°€ ë„ˆë¬´ ë§ì•„ì„œ í•™ìŠµì´ ì˜¤ë˜ê±¸ë ¤ 1/10 ìœ¼ë¡œ ì¤„ì—¬ì„œ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "(ì´ ì½”ë“œëŠ” ìƒ˜í”Œì´ë¯€ë¡œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af26fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.head(int(len(train_data)/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2022d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.head(int(len(test_data)/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89741a",
   "metadata": {},
   "source": [
    "í•™ìŠµì— ì‚¬ìš©ë  pre-trained ëœ BERT ëª¨ë¸ì„ ê°€ì ¸ì™€ì„œ í† í°í™” í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "069729f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name=\"beomi/kcbert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43b8c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56135201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²½ê³ ê°€ ëœ¬ë‹¤ë©´ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "907b89b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37c7c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_sentences = tokenizer(\n",
    "    list(train_data.texts),\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26cadf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_sentences = tokenizer(\n",
    "    list(test_data.texts),\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1403d",
   "metadata": {},
   "source": [
    "ì¶œë ¥í•´ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d59edf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "tensor([[    2,  2170,   832,  ...,     0,     0,     0],\n",
      "        [    2,  3521,    17,  ...,     0,     0,     0],\n",
      "        [    2,  8069,  4089,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2,    43, 17697,  ...,     0,     0,     0],\n",
      "        [    2,  2477,  4116,  ...,     0,     0,     0],\n",
      "        [    2,  2170,  4565,  ...,     0,     0,     0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_sentences.keys())\n",
    "print(tokenized_train_sentences['input_ids'])\n",
    "print(tokenized_train_sentences['attention_mask'])\n",
    "print(tokenized_train_sentences['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1978d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_data['labels'].values\n",
    "test_label = test_data['labels'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f00c2",
   "metadata": {},
   "source": [
    "ë°ì´í„° ë¡œë” ì¤€ë¹„, ì´ê²Œ í•„ìš”í•œ ì´ìœ ëŠ” ë°°ì¹˜ ì²˜ë¦¬í•˜ëŠ” ë‚´ë¶€ì—ì„œ ì›ì†Œë¥¼ ì•¡ì„¸ìŠ¤ í•˜ê¸° ìœ„í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "099da9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataloaderDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a094cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataloaderDataset(tokenized_train_sentences, train_label)\n",
    "test_dataset = DataloaderDataset(tokenized_test_sentences, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b2afc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ab6e455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce8a3eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_config = BertConfig.from_pretrained(\n",
    "    pretrained_model_name,\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        pretrained_model_name,\n",
    "        config=pretrained_model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6884b3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"beomi/kcbert-base\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 300,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.10.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7060de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate\n",
    "#!pip install scikit-learn\n",
    "import numpy as np\n",
    "import evaluate \n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d31c3308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    #per_device_train_batch_size=32,  # batch size per device during training\n",
    "    #per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=100,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    save_on_each_node=True,\n",
    "    do_train=True,                   # Perform training\n",
    "    do_eval=True,                    # Perform evaluation\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    seed=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b7e6b",
   "metadata": {},
   "source": [
    "ì¢€ ë” ë§ì€ ì¸ìëŠ” ì•„ë˜ ë§í¬ì—ì„œ í™•ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe80e3a",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/v4.19.2/en/main_classes/trainer#transformers.TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c0e89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cde846d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 938\n",
      "C:\\Users\\jun\\AppData\\Local\\Temp\\ipykernel_27736\\1263192275.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='938' max='938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [938/938 04:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.344700</td>\n",
       "      <td>0.314100</td>\n",
       "      <td>0.867800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-200\n",
      "Configuration saved in ./results\\checkpoint-200\\config.json\n",
      "Model weights saved in ./results\\checkpoint-200\\pytorch_model.bin\n",
      "C:\\Users\\jun\\AppData\\Local\\Temp\\ipykernel_27736\\1263192275.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Saving model checkpoint to ./results\\checkpoint-400\n",
      "Configuration saved in ./results\\checkpoint-400\\config.json\n",
      "Model weights saved in ./results\\checkpoint-400\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-500] due to args.save_total_limit\n",
      "C:\\Users\\jun\\AppData\\Local\\Temp\\ipykernel_27736\\1263192275.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Saving model checkpoint to ./results\\checkpoint-600\n",
      "Configuration saved in ./results\\checkpoint-600\\config.json\n",
      "Model weights saved in ./results\\checkpoint-600\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-200] due to args.save_total_limit\n",
      "C:\\Users\\jun\\AppData\\Local\\Temp\\ipykernel_27736\\1263192275.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Saving model checkpoint to ./results\\checkpoint-800\n",
      "Configuration saved in ./results\\checkpoint-800\\config.json\n",
      "Model weights saved in ./results\\checkpoint-800\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-400] due to args.save_total_limit\n",
      "C:\\Users\\jun\\AppData\\Local\\Temp\\ipykernel_27736\\1263192275.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=938, training_loss=0.3618158074075988, metrics={'train_runtime': 283.118, 'train_samples_per_second': 52.981, 'train_steps_per_second': 3.313, 'total_flos': 824791491900000.0, 'train_loss': 0.3618158074075988, 'epoch': 1.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03ad7707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to trained_model\n",
      "Configuration saved in trained_model\\config.json\n",
      "Model weights saved in trained_model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7624ab2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
