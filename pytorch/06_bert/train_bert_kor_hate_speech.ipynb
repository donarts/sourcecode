{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02fbfd76",
   "metadata": {},
   "source": [
    "## MultiClass train 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbf437",
   "metadata": {},
   "source": [
    "### 종속 변수(y)에 따른 분류\n",
    "- Binary Class\n",
    "  - 단순히 0 또는 1 [0,1,0,...]\n",
    "- Multi Class\n",
    "  - 0 ~ k개까지 존재하고 이 중 한가지 class로 분류하는 것. [[0,1,0],[1,0,0],...]\n",
    "- Multi Label\n",
    "  - 0 ~ k까지 존재하며, 이 중 최대 k개 까지 중복된 class가 될 수 있는 것. [[1,1,0],[1,0,1],...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d34183",
   "metadata": {},
   "source": [
    "multiclass를 위해서 적절한 데이터를 찾기가 힘들었습니다. 샘플이니 데이터가 좀 작았으면 해서 Kopora의 korean_hate_speech data를 사용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7d55f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Korpora import Korpora\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a09a8f5",
   "metadata": {},
   "source": [
    "fineturning할 Multiclass 데이터를 가져옵니다. hate 항목이 3가지로 나뉨, 적절한 예제로 선택함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23a30df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a05b455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Authors :\n",
      "        - Jihyung Moon* (inmoonlight@github)\n",
      "        - Won Ik Cho* (warnikchow@github)\n",
      "        - Junbum Lee (beomi@github)\n",
      "        * equal contribution\n",
      "    Repository : https://github.com/kocohub/korean-hate-speech\n",
      "    References :\n",
      "        - Moon, J., Cho, W. I., & Lee, J. (2020). BEEP! Korean Corpus of Online News\n",
      "          Comments for Toxic Speech Detection. arXiv preprint arXiv:2005.12503.\n",
      "\n",
      "    We provide the first human-annotated Korean corpus for toxic speech detection and the large unlabeled corpus.\n",
      "    The data is comments from the Korean entertainment news aggregation platform.\n",
      "\n",
      "    # License\n",
      "    Creative Commons Attribution-ShareAlike 4.0 International License.\n",
      "    Visit here for detail : https://creativecommons.org/licenses/by-sa/4.0/\n",
      "\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\jun\\Korpora\\korean_hate_speech\\unlabeled\\unlabeled_comments_1.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\jun\\Korpora\\korean_hate_speech\\unlabeled\\unlabeled_comments_2.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\jun\\Korpora\\korean_hate_speech\\unlabeled\\unlabeled_comments_3.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\jun\\Korpora\\korean_hate_speech\\unlabeled\\unlabeled_comments_4.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\jun\\Korpora\\korean_hate_speech\\unlabeled\\unlabeled_comments_5.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\jun\\Korpora\\korean_hate_speech\\news_title\\unlabeled_comments.news_title_1.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\jun\\Korpora\\korean_hate_speech\\news_title\\unlabeled_comments.news_title_2.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\jun\\Korpora\\korean_hate_speech\\news_title\\unlabeled_comments.news_title_3.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\jun\\Korpora\\korean_hate_speech\\news_title\\unlabeled_comments.news_title_4.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\jun\\Korpora\\korean_hate_speech\\news_title\\unlabeled_comments.news_title_5.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\jun\\Korpora\\korean_hate_speech\\news_title\\dev.news_title.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\jun\\Korpora\\korean_hate_speech\\news_title\\test.news_title.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\jun\\Korpora\\korean_hate_speech\\news_title\\train.news_title.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\jun\\Korpora\\korean_hate_speech\\labeled\\dev.tsv\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\jun\\Korpora\\korean_hate_speech\\labeled\\train.tsv\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\jun\\Korpora\\korean_hate_speech\\test.no_label.tsv\n"
     ]
    }
   ],
   "source": [
    "korean_hate_speech = Korpora.load('korean_hate_speech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f83c1bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KoreanHateSpeech.train: size=7896\n",
       "  - KoreanHateSpeech.train.texts : list[str]\n",
       "  - KoreanHateSpeech.train.titles : list[str]\n",
       "  - KoreanHateSpeech.train.gender_biases : list[str]\n",
       "  - KoreanHateSpeech.train.biases : list[str]\n",
       "  - KoreanHateSpeech.train.hates : list[str]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_hate_speech.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecf0ce1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KoreanHateSpeech.dev: size=471\n",
       "  - KoreanHateSpeech.dev.texts : list[str]\n",
       "  - KoreanHateSpeech.dev.titles : list[str]\n",
       "  - KoreanHateSpeech.dev.gender_biases : list[str]\n",
       "  - KoreanHateSpeech.dev.biases : list[str]\n",
       "  - KoreanHateSpeech.dev.hates : list[str]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_hate_speech.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7476864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "913825b6",
   "metadata": {},
   "source": [
    "dataframe 에 넣어봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5efaf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({\"texts\":korean_hate_speech.train.texts, \"titles\":korean_hate_speech.train.titles,\n",
    "                          \"gender_biases\":korean_hate_speech.train.gender_biases, \"biases\":korean_hate_speech.train.biases,\n",
    "                          \"hates\":korean_hate_speech.train.hates})\n",
    "test_data = pd.DataFrame({\"texts\":korean_hate_speech.dev.texts, \"titles\":korean_hate_speech.dev.titles,\n",
    "                          \"gender_biases\":korean_hate_speech.dev.gender_biases, \"biases\":korean_hate_speech.dev.biases,\n",
    "                          \"hates\":korean_hate_speech.dev.hates})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ce91d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a09fec23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>titles</th>\n",
       "      <th>gender_biases</th>\n",
       "      <th>biases</th>\n",
       "      <th>hates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(현재 호텔주인 심정) 아18 난 마른하늘에 날벼락맞고 호텔망하게생겼는데 누군 계속...</td>\n",
       "      <td>\"밤새 조문 행렬…故 전미선, 동료들이 그리워하는 따뜻한 배우 [종합]\"</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>....한국적인 미인의 대표적인 분...너무나 곱고아름다운모습...그모습뒤의 슬픔을...</td>\n",
       "      <td>\"'연중' 故 전미선, 생전 마지막 미공개 인터뷰…환하게 웃는 모습 '먹먹'[종합]\"</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...못된 넘들...남의 고통을 즐겼던 넘들..이젠 마땅한 처벌을 받아야지..,그래...</td>\n",
       "      <td>\"[단독] 잔나비, 라디오 출연 취소→'한밤' 방송 연기..비판 여론 ing(종합)\"</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1,2화 어설펐는데 3,4화 지나서부터는 갈수록 너무 재밌던데</td>\n",
       "      <td>\"'아스달 연대기' 장동건-김옥빈, 들끓는 '욕망커플'→눈물범벅 '칼끝 대립'\"</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1. 사람 얼굴 손톱으로 긁은것은 인격살해이고2. 동영상이 몰카냐? 메걸리안들 생각...</td>\n",
       "      <td>[DA:이슈] ‘구하라 비보’ 최종범 항소심에 영향?…법조계 “‘공소권 없음’ 아냐”</td>\n",
       "      <td>True</td>\n",
       "      <td>gender</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7891</th>\n",
       "      <td>힘내세요~ 응원합니다!!</td>\n",
       "      <td>\"허지웅, 허투루 넘길 말 없었다…솔직하게 드러냈던 속사정\"</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7892</th>\n",
       "      <td>힘내세요~~삼가 고인의 명복을 빕니다..</td>\n",
       "      <td>\"이혜경, ‘오! 캐롤’ 공연 중 남편 오정욱 부고…오열 속 발인 [종합]\"</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7893</th>\n",
       "      <td>힘내세용 ^^ 항상 응원합니닷 ^^ !</td>\n",
       "      <td>\"'설경구♥' 송윤아, 아들과 즐거운 하루 \"\"전 엄마니까요\"\"\"</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7894</th>\n",
       "      <td>힘내소...연기로 답해요.나도 53살 인데 이런일 저런일 다 있더라구요.인격을 믿습...</td>\n",
       "      <td>\"[SC현장]\"\"연예인 인생 협박 유감\"\"…미소잃은 최민수, '보복운전 혐의' 2차...</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7895</th>\n",
       "      <td>힘들면 관뒀어야지 그게 현명한거다</td>\n",
       "      <td>\"[단독]스태프 사망사고 '서른이지만', 결국 오늘 촬영 취소\"</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7896 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  texts  \\\n",
       "0     (현재 호텔주인 심정) 아18 난 마른하늘에 날벼락맞고 호텔망하게생겼는데 누군 계속...   \n",
       "1     ....한국적인 미인의 대표적인 분...너무나 곱고아름다운모습...그모습뒤의 슬픔을...   \n",
       "2     ...못된 넘들...남의 고통을 즐겼던 넘들..이젠 마땅한 처벌을 받아야지..,그래...   \n",
       "3                    1,2화 어설펐는데 3,4화 지나서부터는 갈수록 너무 재밌던데   \n",
       "4     1. 사람 얼굴 손톱으로 긁은것은 인격살해이고2. 동영상이 몰카냐? 메걸리안들 생각...   \n",
       "...                                                 ...   \n",
       "7891                                      힘내세요~ 응원합니다!!   \n",
       "7892                             힘내세요~~삼가 고인의 명복을 빕니다..   \n",
       "7893                              힘내세용 ^^ 항상 응원합니닷 ^^ !   \n",
       "7894  힘내소...연기로 답해요.나도 53살 인데 이런일 저런일 다 있더라구요.인격을 믿습...   \n",
       "7895                                 힘들면 관뒀어야지 그게 현명한거다   \n",
       "\n",
       "                                                 titles gender_biases  biases  \\\n",
       "0              \"밤새 조문 행렬…故 전미선, 동료들이 그리워하는 따뜻한 배우 [종합]\"         False  others   \n",
       "1       \"'연중' 故 전미선, 생전 마지막 미공개 인터뷰…환하게 웃는 모습 '먹먹'[종합]\"         False    none   \n",
       "2       \"[단독] 잔나비, 라디오 출연 취소→'한밤' 방송 연기..비판 여론 ing(종합)\"         False    none   \n",
       "3          \"'아스달 연대기' 장동건-김옥빈, 들끓는 '욕망커플'→눈물범벅 '칼끝 대립'\"         False    none   \n",
       "4       [DA:이슈] ‘구하라 비보’ 최종범 항소심에 영향?…법조계 “‘공소권 없음’ 아냐”          True  gender   \n",
       "...                                                 ...           ...     ...   \n",
       "7891                  \"허지웅, 허투루 넘길 말 없었다…솔직하게 드러냈던 속사정\"         False    none   \n",
       "7892         \"이혜경, ‘오! 캐롤’ 공연 중 남편 오정욱 부고…오열 속 발인 [종합]\"         False    none   \n",
       "7893               \"'설경구♥' 송윤아, 아들과 즐거운 하루 \"\"전 엄마니까요\"\"\"         False    none   \n",
       "7894  \"[SC현장]\"\"연예인 인생 협박 유감\"\"…미소잃은 최민수, '보복운전 혐의' 2차...         False    none   \n",
       "7895                \"[단독]스태프 사망사고 '서른이지만', 결국 오늘 촬영 취소\"         False    none   \n",
       "\n",
       "     hates  \n",
       "0     hate  \n",
       "1     none  \n",
       "2     hate  \n",
       "3     none  \n",
       "4     hate  \n",
       "...    ...  \n",
       "7891  none  \n",
       "7892  none  \n",
       "7893  none  \n",
       "7894  none  \n",
       "7895  none  \n",
       "\n",
       "[7896 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fccb66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>titles</th>\n",
       "      <th>gender_biases</th>\n",
       "      <th>biases</th>\n",
       "      <th>hates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>송중기 시대극은 믿고본다. 첫회 신선하고 좋았다.</td>\n",
       "      <td>\"'아스달 연대기' 뇌안탈 후계자 송중기, 이아르크에 숨어들었다 [종합]\"</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>지현우 나쁜놈</td>\n",
       "      <td>‘섹션TV’ 유인나 “결혼은 3년 후쯤 했으면…노력해보겠다”</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>알바쓰고많이만들면되지 돈욕심없으면골목식당왜나온겨 기댕기게나하고 산에가서팔어라</td>\n",
       "      <td>\"'골목식당' 포방터시장, 계속되는 논란의 중심[SS초점]\"</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>설마 ㅈ 현정 작가 아니지??</td>\n",
       "      <td>\"\"\"마음 고생\"\"…나영석·정유미, 황당 불륜 지라시→방송작가 검거→벌금형 선고 [...</td>\n",
       "      <td>True</td>\n",
       "      <td>gender</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>이미자씨 송혜교씨 돈이 그리 많으면 탈세말고 그돈으로 평소에 불우이웃에게 기부도 좀...</td>\n",
       "      <td>\"이미자 탈세, 10년간 44억 넘는 소득 신고 누락…약 20억 세금 추가 납부\"</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>지현우 범죄 저지르지 않았나요?</td>\n",
       "      <td>\"[SC현장] '사생결단' 이시영♥지현우, 8년만의 호르몬 로맨스 통할까(종합)\"</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>여자인생 망칠 일 있나 ㅋㅋ</td>\n",
       "      <td>\"박성광, 5월 2일 결혼...\"\"예비신부는 7살 연하 비연예인\"\"(공식)\"</td>\n",
       "      <td>True</td>\n",
       "      <td>gender</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>근데 전라도에서 사고가 났는데 굳이 서울까지 와서 병원에 가느 이유는?</td>\n",
       "      <td>\"[POP이슈]\"\"차량 반파 교통사고\"\"…송가인, 목·허리 통증→정밀검사 진행(종합)\"</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>할매젖x, 뱃살x, 몸매 s라인, 유륜은 적당해야됨(너무크거나 너무 작아도 x), ...</td>\n",
       "      <td>\"[인터뷰①] 수애 \"\"노출·베드신多 장르, 부담보다 도전이라 생각했다\"\"\"</td>\n",
       "      <td>True</td>\n",
       "      <td>gender</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>남자가 잘못한거라면... 반성도 없다면...나였다면 ... 여자처럼 아주 못되게 할...</td>\n",
       "      <td>\"'안재현과 갈등' 구혜선, SNS 활동 재개…\"\"다시 시작\"\" [종합]\"</td>\n",
       "      <td>True</td>\n",
       "      <td>gender</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>471 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 texts  \\\n",
       "0                          송중기 시대극은 믿고본다. 첫회 신선하고 좋았다.   \n",
       "1                                              지현우 나쁜놈   \n",
       "2           알바쓰고많이만들면되지 돈욕심없으면골목식당왜나온겨 기댕기게나하고 산에가서팔어라   \n",
       "3                                     설마 ㅈ 현정 작가 아니지??   \n",
       "4    이미자씨 송혜교씨 돈이 그리 많으면 탈세말고 그돈으로 평소에 불우이웃에게 기부도 좀...   \n",
       "..                                                 ...   \n",
       "466                                  지현우 범죄 저지르지 않았나요?   \n",
       "467                                    여자인생 망칠 일 있나 ㅋㅋ   \n",
       "468            근데 전라도에서 사고가 났는데 굳이 서울까지 와서 병원에 가느 이유는?   \n",
       "469  할매젖x, 뱃살x, 몸매 s라인, 유륜은 적당해야됨(너무크거나 너무 작아도 x), ...   \n",
       "470  남자가 잘못한거라면... 반성도 없다면...나였다면 ... 여자처럼 아주 못되게 할...   \n",
       "\n",
       "                                                titles gender_biases  biases  \\\n",
       "0            \"'아스달 연대기' 뇌안탈 후계자 송중기, 이아르크에 숨어들었다 [종합]\"         False    none   \n",
       "1                    ‘섹션TV’ 유인나 “결혼은 3년 후쯤 했으면…노력해보겠다”         False    none   \n",
       "2                    \"'골목식당' 포방터시장, 계속되는 논란의 중심[SS초점]\"         False    none   \n",
       "3    \"\"\"마음 고생\"\"…나영석·정유미, 황당 불륜 지라시→방송작가 검거→벌금형 선고 [...          True  gender   \n",
       "4        \"이미자 탈세, 10년간 44억 넘는 소득 신고 누락…약 20억 세금 추가 납부\"         False    none   \n",
       "..                                                 ...           ...     ...   \n",
       "466      \"[SC현장] '사생결단' 이시영♥지현우, 8년만의 호르몬 로맨스 통할까(종합)\"         False    none   \n",
       "467         \"박성광, 5월 2일 결혼...\"\"예비신부는 7살 연하 비연예인\"\"(공식)\"          True  gender   \n",
       "468   \"[POP이슈]\"\"차량 반파 교통사고\"\"…송가인, 목·허리 통증→정밀검사 진행(종합)\"         False  others   \n",
       "469         \"[인터뷰①] 수애 \"\"노출·베드신多 장르, 부담보다 도전이라 생각했다\"\"\"          True  gender   \n",
       "470          \"'안재현과 갈등' 구혜선, SNS 활동 재개…\"\"다시 시작\"\" [종합]\"          True  gender   \n",
       "\n",
       "         hates  \n",
       "0         none  \n",
       "1    offensive  \n",
       "2         hate  \n",
       "3         hate  \n",
       "4    offensive  \n",
       "..         ...  \n",
       "466  offensive  \n",
       "467       hate  \n",
       "468  offensive  \n",
       "469       hate  \n",
       "470       none  \n",
       "\n",
       "[471 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1f30f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hates    \n",
       "none         3486\n",
       "offensive    2499\n",
       "hate         1911\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[['hates']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33fc280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2584c0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(l) for l in train_data['texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c9201e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(l) for l in test_data['texts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89741a",
   "metadata": {},
   "source": [
    "학습에 사용될 pre-trained 된 BERT 모델을 가져와서 토큰화 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "069729f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name=\"beomi/kcbert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43b8c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56135201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경고가 뜬다면 다음 명령으로 설치해주자 !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "907b89b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37c7c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_sentences = tokenizer(\n",
    "    list(train_data.texts),\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26cadf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_sentences = tokenizer(\n",
    "    list(test_data.texts),\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1403d",
   "metadata": {},
   "source": [
    "출력해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d59edf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "tensor([[    2,    11,  8979,  ...,     0,     0,     0],\n",
      "        [    2,    17,    17,  ...,     0,     0,     0],\n",
      "        [    2,    17,    17,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2,  9104,  4066,  ...,     0,     0,     0],\n",
      "        [    2,  9104,  4266,  ...,     0,     0,     0],\n",
      "        [    2, 24825,   323,  ...,     0,     0,     0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_sentences.keys())\n",
    "print(tokenized_train_sentences['input_ids'])\n",
    "print(tokenized_train_sentences['attention_mask'])\n",
    "print(tokenized_train_sentences['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fd28624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7896, 74])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_sentences['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79b6901d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hates_hate</th>\n",
       "      <th>hates_none</th>\n",
       "      <th>hates_offensive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7891</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7892</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7893</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7894</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7895</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7896 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hates_hate  hates_none  hates_offensive\n",
       "0              1           0                0\n",
       "1              0           1                0\n",
       "2              1           0                0\n",
       "3              0           1                0\n",
       "4              1           0                0\n",
       "...          ...         ...              ...\n",
       "7891           0           1                0\n",
       "7892           0           1                0\n",
       "7893           0           1                0\n",
       "7894           0           1                0\n",
       "7895           0           1                0\n",
       "\n",
       "[7896 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels=pd.get_dummies(train_data[['hates']])\n",
    "train_labels # hate:[1,0,0], none:[0,1,0], offensive:[0,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ce303e",
   "metadata": {},
   "source": [
    "one hot encoding 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60f8d2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hates_hate</th>\n",
       "      <th>hates_none</th>\n",
       "      <th>hates_offensive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>471 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     hates_hate  hates_none  hates_offensive\n",
       "0             0           1                0\n",
       "1             0           0                1\n",
       "2             1           0                0\n",
       "3             1           0                0\n",
       "4             0           0                1\n",
       "..          ...         ...              ...\n",
       "466           0           0                1\n",
       "467           1           0                0\n",
       "468           0           0                1\n",
       "469           1           0                0\n",
       "470           0           1                0\n",
       "\n",
       "[471 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels=pd.get_dummies(test_data[['hates']])\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "201178bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3a60155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af6d54",
   "metadata": {},
   "source": [
    "멀티 Class 일때 label shape가 제대로 생성되었는지 확인하도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1978d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_labels.values.astype(float) # 꼭 float 로 변환해 줍니다.\n",
    "test_label = test_labels.values.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f00c2",
   "metadata": {},
   "source": [
    "데이터 로더 준비, 이것이 필요한 이유는 배치 처리하는 내부에서 원소를 액세스 하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "099da9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataloaderDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a094cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataloaderDataset(tokenized_train_sentences, train_label)\n",
    "test_dataset = DataloaderDataset(tokenized_test_sentences, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b2afc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ab6e455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce8a3eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_config = BertConfig.from_pretrained(\n",
    "    pretrained_model_name,\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        pretrained_model_name,\n",
    "        #config=pretrained_model_config,\n",
    "        num_labels=3,\n",
    "        #problem_type=\"multi_label_classification\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109ff5ea",
   "metadata": {},
   "source": [
    "labels 에 float 형이 들어가 있으므로 np.argmax 로 동일하게 정수형으로 출력되도록 해줍니다. labels_ = np.argmax(labels, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7060de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate\n",
    "#!pip install scikit-learn\n",
    "import numpy as np\n",
    "import evaluate \n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    labels_ = np.argmax(labels, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d31c3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    #per_device_train_batch_size=32,  # batch size per device during training\n",
    "    #per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    per_device_train_batch_size=5,  # batch size per device during training\n",
    "    per_device_eval_batch_size=5,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=100,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    save_on_each_node=True,\n",
    "    do_train=True,                   # Perform training\n",
    "    do_eval=True,                    # Perform evaluation\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    seed=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe80e3a",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/v4.19.2/en/main_classes/trainer#transformers.TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c0e89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cde846d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 7896\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 5\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 5\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1580\n",
      "C:\\Users\\jun\\AppData\\Local\\Temp\\ipykernel_29796\\1263192275.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1580' max='1580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1580/1580 02:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.489900</td>\n",
       "      <td>0.463366</td>\n",
       "      <td>0.683652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-200\n",
      "Configuration saved in ./results\\checkpoint-200\\config.json\n",
      "Model weights saved in ./results\\checkpoint-200\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-1200] due to args.save_total_limit\n",
      "C:\\Users\\jun\\AppData\\Local\\Temp\\ipykernel_29796\\1263192275.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Saving model checkpoint to ./results\\checkpoint-400\n",
      "Configuration saved in ./results\\checkpoint-400\\config.json\n",
      "Model weights saved in ./results\\checkpoint-400\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-1400] due to args.save_total_limit\n",
      "C:\\Users\\jun\\AppData\\Local\\Temp\\ipykernel_29796\\1263192275.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Saving model checkpoint to ./results\\checkpoint-600\n",
      "Configuration saved in ./results\\checkpoint-600\\config.json\n",
      "Model weights saved in ./results\\checkpoint-600\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-200] due to args.save_total_limit\n",
      "C:\\Users\\jun\\AppData\\Local\\Temp\\ipykernel_29796\\1263192275.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Saving model checkpoint to ./results\\checkpoint-800\n",
      "Configuration saved in ./results\\checkpoint-800\\config.json\n",
      "Model weights saved in ./results\\checkpoint-800\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-400] due to args.save_total_limit\n",
      "C:\\Users\\jun\\AppData\\Local\\Temp\\ipykernel_29796\\1263192275.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-600] due to args.save_total_limit\n",
      "C:\\Users\\jun\\AppData\\Local\\Temp\\ipykernel_29796\\1263192275.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Saving model checkpoint to ./results\\checkpoint-1200\n",
      "Configuration saved in ./results\\checkpoint-1200\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1200\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-800] due to args.save_total_limit\n",
      "C:\\Users\\jun\\AppData\\Local\\Temp\\ipykernel_29796\\1263192275.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Saving model checkpoint to ./results\\checkpoint-1400\n",
      "Configuration saved in ./results\\checkpoint-1400\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1400\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-1000] due to args.save_total_limit\n",
      "C:\\Users\\jun\\AppData\\Local\\Temp\\ipykernel_29796\\1263192275.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 471\n",
      "  Batch size = 5\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1580, training_loss=0.5128312267834627, metrics={'train_runtime': 163.0519, 'train_samples_per_second': 48.426, 'train_steps_per_second': 9.69, 'total_flos': 300269965687776.0, 'train_loss': 0.5128312267834627, 'epoch': 1.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41e4aa4",
   "metadata": {},
   "source": [
    "1 epoch에 정확도가 68%정도 나옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03ad7707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to trained_model_hate\n",
      "Configuration saved in trained_model_hate\\config.json\n",
      "Model weights saved in trained_model_hate\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"trained_model_hate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acca540",
   "metadata": {},
   "source": [
    "환경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b09401ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==1.4.0\n",
      "aiohttp==3.8.4\n",
      "aiosignal==1.3.1\n",
      "anyio==3.6.2\n",
      "argon2-cffi==21.3.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.2.3\n",
      "asttokens==2.2.1\n",
      "async-timeout==4.0.2\n",
      "attrs==22.2.0\n",
      "backcall==0.2.0\n",
      "beautifulsoup4==4.11.2\n",
      "bleach==6.0.0\n",
      "cachetools==5.3.0\n",
      "certifi==2022.12.7\n",
      "cffi==1.15.1\n",
      "charset-normalizer==3.1.0\n",
      "click==8.1.3\n",
      "colorama==0.4.6\n",
      "comm==0.1.2\n",
      "dataclasses==0.6\n",
      "datasets==2.10.1\n",
      "debugpy==1.6.6\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "dill==0.3.6\n",
      "evaluate==0.4.0\n",
      "executing==1.2.0\n",
      "fastjsonschema==2.16.3\n",
      "filelock==3.9.0\n",
      "Flask==2.2.3\n",
      "Flask-Cors==3.0.10\n",
      "flask-ngrok==0.0.25\n",
      "fqdn==1.5.1\n",
      "frozenlist==1.3.3\n",
      "fsspec==2023.3.0\n",
      "google-auth==2.16.2\n",
      "google-auth-oauthlib==0.4.6\n",
      "grpcio==1.51.3\n",
      "huggingface-hub==0.13.0\n",
      "idna==3.4\n",
      "ipykernel==6.21.3\n",
      "ipython==8.11.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==8.0.4\n",
      "isoduration==20.11.0\n",
      "itsdangerous==2.1.2\n",
      "jedi==0.18.2\n",
      "Jinja2==3.1.2\n",
      "joblib==1.2.0\n",
      "jsonpointer==2.3\n",
      "jsonschema==4.17.3\n",
      "jupyter-events==0.6.3\n",
      "jupyter_client==8.0.3\n",
      "jupyter_core==5.2.0\n",
      "jupyter_server==2.4.0\n",
      "jupyter_server_terminals==0.4.4\n",
      "jupyterlab-pygments==0.2.2\n",
      "jupyterlab-widgets==3.0.5\n",
      "Korpora==0.2.0\n",
      "Markdown==3.4.1\n",
      "MarkupSafe==2.1.2\n",
      "matplotlib-inline==0.1.6\n",
      "mistune==2.0.5\n",
      "multidict==6.0.4\n",
      "multiprocess==0.70.14\n",
      "nbclassic==0.5.3\n",
      "nbclient==0.7.2\n",
      "nbconvert==7.2.9\n",
      "nbformat==5.7.3\n",
      "nest-asyncio==1.5.6\n",
      "notebook==6.5.3\n",
      "notebook_shim==0.2.2\n",
      "numpy==1.24.2\n",
      "oauthlib==3.2.2\n",
      "packaging==23.0\n",
      "pandas==1.5.3\n",
      "pandocfilters==1.5.0\n",
      "parso==0.8.3\n",
      "pickleshare==0.7.5\n",
      "Pillow==9.4.0\n",
      "platformdirs==3.1.1\n",
      "prometheus-client==0.16.0\n",
      "prompt-toolkit==3.0.38\n",
      "protobuf==4.22.1\n",
      "psutil==5.9.4\n",
      "pure-eval==0.2.2\n",
      "pyarrow==11.0.0\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycparser==2.21\n",
      "pyDeprecate==0.3.2\n",
      "Pygments==2.14.0\n",
      "pyrsistent==0.19.3\n",
      "python-dateutil==2.8.2\n",
      "python-json-logger==2.0.7\n",
      "pytorch-lightning==1.6.1\n",
      "pytz==2022.7.1\n",
      "pywin32==305\n",
      "pywinpty==2.0.10\n",
      "PyYAML==6.0\n",
      "pyzmq==25.0.0\n",
      "ratsnlp==1.0.52\n",
      "regex==2022.10.31\n",
      "requests==2.28.2\n",
      "requests-oauthlib==1.3.1\n",
      "responses==0.18.0\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rsa==4.9\n",
      "sacremoses==0.0.53\n",
      "scikit-learn==1.2.2\n",
      "scipy==1.10.1\n",
      "Send2Trash==1.8.0\n",
      "six==1.16.0\n",
      "sniffio==1.3.0\n",
      "soupsieve==2.4\n",
      "stack-data==0.6.2\n",
      "tensorboard==2.12.0\n",
      "tensorboard-data-server==0.7.0\n",
      "tensorboard-plugin-wit==1.8.1\n",
      "terminado==0.17.1\n",
      "threadpoolctl==3.1.0\n",
      "tinycss2==1.2.1\n",
      "tokenizers==0.10.3\n",
      "torch==1.13.1+cu116\n",
      "torchaudio==0.13.1\n",
      "torchmetrics==0.11.3\n",
      "torchvision==0.14.1\n",
      "tornado==6.2\n",
      "tqdm==4.65.0\n",
      "traitlets==5.9.0\n",
      "transformers==4.10.0\n",
      "typing_extensions==4.5.0\n",
      "uri-template==1.2.0\n",
      "urllib3==1.26.14\n",
      "wcwidth==0.2.6\n",
      "webcolors==1.12\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.5.1\n",
      "Werkzeug==2.2.3\n",
      "widgetsnbextension==4.0.5\n",
      "xlrd==2.0.1\n",
      "xxhash==3.2.0\n",
      "yarl==1.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f41cdcc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.10.10'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.python_version()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
